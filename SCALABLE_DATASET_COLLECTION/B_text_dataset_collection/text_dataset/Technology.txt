The March issue of IEEE Spectrum is here! IEEE is a global community of more than 400,000 engineers, scientists, and allied professionals working together to advance technology for the benefit of humanity. Learn more about becoming an IEEE Member here. IEEE Spectrum is the flagship publication of the IEEE, and IEEE Members receive a full subscription to IEEE Spectrum 12 issues per year, either in print or digital. To become an IEEE Member and enjoy a subscription to IEEE Spectrum, among many other benefits, please click here. We also offer a standalone subscription to IEEE Spectrum through our Subscription Service provider Omeda. Want to renew your IEEE Spectrum subscription or need to update your mailing address? To manage your cookie preferences, please click here Cookie Preferences IEEE Spectrum enjoys a large and influential readership of leading engineers and scientists who are building the future of technology. IEEE Spectrum offers a wide range of advertising and marketing opportunities in IEEE Spectrum Magazine, on the IEEE Spectrum website, as well as in other IEEE Spectrum products, including Newsletters, Sponsored Articles, Special Reports and Special Issues, Spectrum Collections, Robots Guide, Webinars, Whitepapers, and more. To learn more about these offerings, please visit httpsadvertise.ieee.org To view exclusive content, save articles to read later, engage in conversations with readers and editors, you need to have an IEEE Account and sign in to the IEEE Spectrum site. Creating an IEEE Account is free. If you are an IEEE Member, you dont need to create a new account Use your current IEEE Account username and password to login to IEEE Spectrum. If you are not an IEEE Member, you can create an IEEE Account for free by clicking here. Note that youll be transferred to an IEEE page to complete the account creation, and at the end youll be redirected back to the IEEE Spectrum site. If you already have an IEEE Account and you need to change your email address, access your account here to update your information. To reset your password, visit this page. If you need further assistance with your account, please contact member-servicesieee.org. If you have a comment on our editorial coverage or other editorial inquiries, please contact our Editor-in-Chief or individual members of our staff listed on our About Us page. If you have a comment on a specific article published by IEEE Spectrum, please post your comment directly on the article page, so the authors, editors, and other readers can comment on it. To submit a news tip or article idea to IEEE Spectrum for consideration, please contact our Editor-in-Chief or individual editors on our staff listed on our About Us page. If you are a PR professional and have a story pitch, please look up our individual editors listed on our About Us page and identify the editor who covers the area of interest. For sponsored articles, webinars, whitepapers, and other marketing opportunities, please contact Aviva Rothman from Naylor Association Solutions at 1 352-333-3435 or by email at arothmannaylor.com. If you posted a comment on an article and youre not seeing it on the page, please keep in mind that comments from non-IEEE Members must be approved before they appear on the site. Though we try to approve comments quickly, it may take up to 48 hours from the time you post a comment until it appears on the site. Also, please review our commenting policy below. Our goal is to foster engaging, substantive conversations on the development and advancement of technology, and its societal, economic, and cultural impacts. To enable that, comments are moderated before they appear on our site. We believe in open dialogue and, to the best of our ability, will not censor comments based on political or ideological point of view. However, we reserve the right to not post any comments that in our sole discretion we feel are not appropriate. For example, a comment will not be posted if it Note that this is not a comprehensive list, and IEEE Spectrum editors reserve the right to remove anything we deem inappropriate or not constructive to advancing dialogue. You are solely responsible for ensuring that any material you post complies with the foregoing conditions and will indemnify IEEE Spectrum, its officers and employees from and against all claims, liabilities, judgments, damages, and costs including attorneys fees, which arise out of materials you have posted on the site. If IEEE Spectrum becomes aware of any material that it believes may violate the foregoing conditions, it may, at its sole discretion, modify or remove the material, or take any other steps it deems necessary, including, but not limited to, referral to the IEEE Ethics and Member Conduct Committee. You grant to IEEE Spectrum the right to copy, publish, distribute, modify, translate and otherwise use in any medium any material that you place on our site by any means, without compensation to you. ANY RIGHTS NOT EXPRESSLY GRANTED HEREIN ARE RESERVED. We apologize for any trouble you may have with our site. If you need assistance with a content or technical problem, please visit the IEEE Support Center, where you can find answers to many common questions, as well as contact information for our support team. Please include any information you can provide about the operating system, web browser and version, and the specific problem you are having. Thank you for your interest in our webinars and whitepapers, and we apologize for any problems you are having registering for or accessing this material. Please contact our webinars and whitepapers team at IEEEnaylor.com directly and theyll assist you as soon as possible. For advertising and sponsorship opportunities, please contact Joerg Wuellner and Chris Darch, and view our media kit here. Joerg Wuellner 49 152 2258 4972 jwuellnerwiley.com Chris Darch 1 206 867 4165 cdarchwiley.com Subscribe or resolve common issues with your subscription. Are you an IEEE member? Email member-servicesieee.org to change your mailing address, replace a missing or damaged issue, or add a print or digital subscription. Not an IEEE member? Our Subscription Service provider Omeda will help you update your information, renew your subscription, or answer other subscriptions questions. For reprints and licensing requests, please contact our licensing partner PARS International. Include the full content title, as well as issue dates, page numbers, names of authors and artists, and any other relevant information.

The March issue of IEEE Spectrum is here! Balancing AIs potential with research integrity is critical Wynand Lambrechts is an IEEE senior member and a senior research associate at the University of Johannesburg. Saurabh Sinha is an IEEE Fellow and a professor and executive dean of engineering at the University of Canterbury, in Christchurch, New Zealand. Dorsamy Pillay is an honorary scholar at the International Institute for Applied Systems Analysis, in Laxenburg, Austria. This is a guest post. The views expressed here are the authors own and do not represent positions of IEEE Spectrum, The Institute or IEEE. Scientific writing is at a pivotal stage, driven by artificial intelligence as a disruptor and enabler. Academics, publishers, and policymakers are attempting to weigh the value of using AI responsibly to enhance productivity versus risking the integrity and purpose of scholarly communication. In this context, the responsible use of the technology in scientific writing pertains to employing AI tools in ways that uphold the integrity, transparency, and ethical standards of scholarly communication. As we collectively contend with the challenges and define AIs ethical use, we must ask Is AI revolutionizing scientific writing or undermining it? Technology has long been involved in shaping the scientific writing landscape. Word processors and then personal computers revolutionized how manuscripts were created and shared. The emergence of online submission platforms and open-access repositories further transformed access to knowledge, allowing for large-scale global collaboration and peer review. Modern methods also include alternative metrics that track and analyze the awareness an article generates online to determine where the research is having an impact on social media. From the early digitization of research dissemination to the influence and leverage of social media, challenges to balance progress with quality in writing continue to evolve. AI, especially generative large language models, can draft manuscripts, conduct literature reviews, provide translations, and generate content faster than humans can. The ubiquitous nature and rapid evolution of the advancements, however, require stakeholders to take a step back and consider their ethical and practical limitations and implications. A unified effort within the academic community is needed to ensure that AI in scientific writing is used responsibly to enhance critical thinking, not replace it. This concept aligns with the broader vision of augmented artificial intelligence, advocating for the collaboration between human judgment and AI toward ethical technology development and applying the same principles to scientific writing. Policies and frameworks must stay rooted in the fundamentals of scientific writing advancing knowledge, prioritizing quality over quantity, and fostering transparency and accountability. Excessive use of AI can have the opposite effect and raise concerns over plagiarism and academic integrity, especially as traditional AI detection algorithms require continuous adaptation to stay relevant. Empirical research lays the foundation for identifying AIs limitations and refining detection tools to align the technologys capabilities with ethical standards. Through international collaborative efforts and our shared experiences on the responsible use of AI, we will be able to develop appropriate measures to deal with it in scholarly writing. The challenges AI presents are multidisciplinary and transcend fields. Consistent and collective efforts that address common issues can benefit the entire scientific community. To integrate AI into scientific writing, we advocate for a collaborative effort to maintain ethical standards, promote transparency, and address challenges that arise. Cooperation among academics, publishers, and policymakers, along with industry and government representatives at both national and international levels, is highly recommended. Such a collaborative effort must strive to create or refine digital identification tools for AI-generated content in academia. We propose that the collaborative effort be underpinned by two primary areas the development of global frameworks, policies, and training initiatives to encourage the responsible integration of AI in scientific writing and the creation of AI detection tools rooted in empirical research to serve as a blueprint of ethical standards and guidelines. Journals, for example, should regularly update author guidelines to address the evolving role of AI in scientific writing. Clear policies could explicitly permit AI to assist with grammar and language editing while prohibiting its use for drafting original ideas, hypotheses, or results. It also would level the playing field for people who are not native English speakers. To uphold research integrity and align with the core principles of transparency and accountability, AI tools could be allowed to leverage preliminary data analysis, provided that the datasets and methodologies are open-access and reproducible. Manuscript evaluation and editing processes must continuously be refined to detect potential ethical violations and guide the responsible integration of AI into article submissions. Those approaches, combined with updated journal policies, would help set international best practices that augment human intelligence without replacing critical thinking. Global policies, workshops, and training programs must be developed to further uphold the rigorous standards of scholarship within the scientific community. Such collaborative platforms frequently encourage international dialogue and broaden awareness while strengthening the ethical integration of AI in scientific writing. Joint efforts of ethical impact assessments, community engagement initiatives, and global participation promote alignment with best practices, policy coherence, and transparency. To maintain a successful and broad ethical framework, current AI detection tools are vital for identifying AI-generated content and maintaining academic accountability, despite being inconsistent. When combined with human oversight, the tools effectiveness and efficacy as first-order checks that verify the responsible use of AI can be increased. As AI detection tools continue to evolve, it is crucial to focus on standardizing and refining the technologies through collaborative global efforts. Even if the scientific writing community does not develop or own its own AI detection systems, its influence and involvement are pivotal in shaping its ethical requirements and guidelines. As the tools become standardized and more accurate, their adoption will assist in maintaining ethical standards in scientific writing and enabling cross-disciplinary stakeholders to address shared challenges. To address the ethical complexities of AI integration in scientific writing, a comprehensive and proactive approach is essential. Well-defined frameworks and policies, plus adaptive and robust tools, can oversee the responsible use of AI and support collaboration. Key actions to avoid compromising the integrity of scholarly communication include The challenges of integrating AI into scientific writing mirror the broader ethical complexities of technological innovation. By prioritizing collaboration, transparency, and accountability, the scientific community can ensure that AI becomes a tool for progress rather than a compromise on existing standards. AIs transformative power is undeniable. Its integration into scientific writing must be approached with caution and foresight. By prioritizing ethics and quality, academia can navigate the new arena without compromising the foundational principles of scholarly communication and contribution. The ultimate test lies not in how effectively AI can mimic human intelligence but in how responsibly we harness it to uphold the values of scholarship. Wynand Lambrechts is an IEEE senior member and a senior research associate at the University of Johannesburg. He is a principal electronics engineer at Incomar. Saurabh Sinha is an IEEE Fellow and is a professor and executive dean of engineering at the University of Canterbury, in Christchurch, New Zealand. Dorsamy Pillay is an honorary scholar at the International Institute for Applied Systems Analysis, in Laxenburg, Austria.

The March issue of IEEE Spectrum is here! XoMotion gets people with spinal cord injuries on their feetand dancing Agritech apps are providing personalized advice to small farmers Tiny faults could find big applications for chips, art history, and more Outpace design complexity with shift-left design rule checking methods The company wants to make graphene sensors more accessible to industries The tech could help develop more bat-inspired devices in the future Researchers say factories on Mars could benefit from the efficient approach A synthetic e-skin uses capacitance to detect nearby objects The rapid sensor is as good as smelling as a mouses nose This shoebox-sized helium sensor is tracking gas levels of Italian volcanoes She checks out the sensors and subsystems used by millions The technique makes it easier to monitor 3D-printed living structures Can cameras inside of robots feet help them walk? Sensor networks enable 40 percent more electricity to pulse through the lines Unpacking the complexities of energy system transition Tennessee field trials power sensors for just pennies per day Heart rate, breathing , movement, and temperature sensing in a 4-mm-wide package The latest developments in aviation, satellites, astrophysics, space flight, and space exploration The latest advances in artificial intelligence and machine learning, generative AI, ChatGPT, LLMs, deepfakes, and more The latest developments in biomedical devices, prosthetics, imaging, and diagnostics The latest developments in renewables, carbon capture, emissions monitoring, geoengineering, and electrifying everything The latest developments in embedded systems, hardware, software, quantum computers, and IT The latest developments in gadgets, gaming, smart phones, virtual reality, and audiovideo The latest developments in power generation, batteries, renewables, smart grid, and nuclear energy A look back at key moments in the history of cyberspace, electronics, and the space age The latest developments in consumer robots, humanoids, drones, and automation The latest developments in processors, memory, nanotechnology, optoelectronics, and integrated circuit design and materials The latest developments in computer vision, lidar, chemical sensors, and remote sensing The latest developments in wireless networks, Internet standards, 5G, IoT, security, and information theory The latest developments in electric vehicles, advanced aircraft, and self-driving cars Reviews, hands-on projects, and the latest on the maker movement from Spectrums DIY editor Stephen Cass Profiles and interviews with engineers, tech industry news, and the latest trends on the job market The latest news about IEEE, its members, tech history, and new offerings

The March issue of IEEE Spectrum is here! Its not more bandwidth that users need Is the worldwide race to keep expanding mobile bandwidth a fools errand? Could maximum data speedson mobile devices, at home, at workbe approaching fast enough for most people for most purposes? These heretical questions are worth asking, because industry bandwidth tracking data has lately been revealing something surprising Terrestrial and mobile-data growth is slowing down. In fact, absent a dramatic change in consumer tech and broadband usage patterns, data-rate demand appears set to top out below 1 billion bits per second 1 gigabit per second in just a few years. This is a big deal. A presumption of endless growth in wireless and terrestrial broadband data rates has for decades been a key driver behind telecom research funding. To keep telecoms RD engine rooms revving, research teams around the world have innovated a seemingly endless succession of technologies to expand bandwidth rates, such as 2Gs move to digital cell networks, 3Gs enhanced data-transfer capabilities, and 5Gs low-latency wireless connectivity. Yet present-day consumer usage appears set to throw a spanner in the works. Typical real-world 5G data rates today achieve up to 500 megabits per second for download speeds and less for uploads. And some initial studies suggest 6G networks could one day supply data at 100 Gbs. But the demand side of the equation suggests a very different situation. Mainstream consumer applications requiring more than 1 Gbs border on the nonexistent. This is in part because mobile applications that need more than 15 to 20 Mbs are rare, while mainstream consumer applications requiring more than 1 Gbs border on the nonexistent. At most, meeting the demand for multiple simultaneous active applications and users requires hundreds of Mbs range. To date, no new consumer technologies have emerged to expand the bandwidth margins much beyond the 1 Gbs plateau. Yet wireless companies and researchers today still set their sights on a marketplace where consumer demand will gobble up as much bandwidth as can be provided by their mobile networks. The thinking here seems to be that if more bandwidth is available, new use cases and applications will spontaneously emerge to consume it. Is that such a foregone conclusion, though? Many technologies have had phases where customers eagerly embrace every improvement in some parameteruntil a saturation point is reached and improvements are ultimately met with a collective shrug. Consider a very brief history of airspeed in commercial air travel. Passenger aircraft today fly at around 900 kilometers per hourand have continued to traverse the skies at the same airspeed range for the past five decades. Although supersonic passenger aircraft found a niche from the 1970s through the early 2000s with the Concorde, commercial supersonic transport is no longer available for the mainstream consumer marketplace today. To be clear, there may still be niche use cases for many gigabits per second of wireless bandwidthjust as there may still be executives or world leaders who continue to look forward to spanning the globe at supersonic speeds. But what if the vast majority of 6Gs consumer bandwidth demand ultimately winds up resembling todays 5G profile? Its a possibility worth imagining. Transmitting high-end 4K video today requires 15 Mbs, according to Netflix. Home broadband upgrades from, say, hundreds of Mbs to 1,000 Mbs or 1 Gbs typically make little to no noticeable difference for the average end user. Likewise, for those with good 4G connectivity, 5G makes much less of an improvement on the mobile experience than advertisers like to claimdespite 5G networks being, according to Cisco, 1.4 to 14 times as fast as 4G. So, broadly, for a typical mobile device today, going much above 15 Mbs borders on pointless. For a home, assuming two or three inhabitants all separately browsing or watching, somewhere between 100 Mbs and 1 Gbs marks the approximate saturation point beyond which further improvements become less and less noticeable, for most use cases. Probing a more extreme use case, one of the largest bandwidth requirements in recent consumer tech is Microsoft Flight Simulator 2024, whose jaw-dropping bandwidth demand, in the words of Windows Central, amounts to a maximum of 180 Mbs. Stop to think about that for one moment. Here is a leading-edge tech product requiring less than one-fifth of 1 Gbs, and such a voracious bandwidth appetite today is considered jaw-dropping. But what about the need to future proof the worlds networks? Perhaps most mobile and terrestrial networks dont need many-Gbs connectivity now, say the bigger-is-always-better proponents. But the world will soon! For starters, then, what bandwidth-hogging technologies are today on the horizon? In September, Apple unveiled its iPhone 16, which CEO Tim Cooksaid would feature generative AI broadly across Apple products. Could Apples new AI capabilities perhaps be a looming, bandwidth-consuming dark horse? One high-bandwidth use case would involve the latest iPhone using the camera to recognize a scene and comment on whats in it. However, thats not dissimilar to Google Lenss visual search feature, which hasnt markedly changed network traffic. Indeed, this sort of feature, perhaps used a few times per day, could require bandwidth equivalent to a second or two of high-definition video. None of this would come close to saturating the general bandwidth capacities noted above. To play devils advocate a little more, consider a representative batch of five soon-to-be-scaled-up, potentially high-bandwidth consumer technologies that do already exist. Do any of them appear poised to generate the many-Gbs demand that present-day net usage does not? What about autonomous cars, for instance? Surely theyll need as much bandwidth as they can possibly be given. Yet, the precious few autonomous cars out in the world today are generally designed to work without much in the way of instantaneous Internet communication. And no autonomous tech around the bend appears set to change the equation substantially, concerning instantaneous bandwidth needs. The future of autonomy may be revolutionary and ultimately inevitable, but it doesnt appear to require network connectivity much beyond a decent 5G connection. No new technology has emerged that demands network requirements much beyond what 4G and 5G already deliver. Much the same argument holds for the Internet of things IoT, which is not expected to increase network traffic above what a decent 4G connection could yield. Holographic communications likewise offer no greater bandwidth sink than any of the above case studies do. For a typical user, holograms are in fact just stereographic video projections. So if a single 4K stream demands 15 Mbs, then stereo 4K streams would require 30 Mbs. Of course, sophisticated representations of entire 3D scenes for large groups of users interacting with one another in-world could conceivably push bandwidth requirements up. But at this point, were getting into Matrix-like imagined technologies without any solid evidence to suggest a good 4G or 5G connection wouldnt meet the techs bandwidth demands. AI in general is the wild card in the deck. The mysterious future directions for this technology suggest that AI broadband and wireless bandwidth needs could conceivably exceed 1 Gbs. But consider at least the known knowns in the equation At the moment, present-day AI applications involve small amounts of prompt text or a few images or video clips sent to and from an edge device like a smartphone or a consumer tablet. Even if one allows for the prompt text and photo and video bandwidth requirements to dramatically expand from there, it seems unlikely to match or exceed the already strenuous requirements of a simple 4K video stream. Which, as noted above, would appear to suggest modest bandwidth demands in the range of 15 Mbs. The metaverse, meanwhile, has flopped. But even if it picks up steam again tomorrow, current estimates of its bandwidth needs run from 100 Mbs to 1 Gbsall within 5Gs range. Admittedly, the most aggressive longer-term forecasts for the metaverse suggest that cutting-edge applications could demand as much as 5 Gbs bandwidth. And while its true that in January, Verizon delivered more than 5 Gbs bandwidth in an experimental 5G network, that result is unlikely to be replicable for most consumers in most settings anytime soon. Yet, even allowing for the practical unreachability of 5 Gbs speeds on a real-world 5G network, a reader should still weigh the fact that any such imagined applications that might ultimately consume 5 Gbs of bandwidth represent an extreme. And only the upper end of that subset is what might one day exceed data speeds that present-day 5G tech delivers. I would argue, in other words, that no new technology has emerged that demands network requirements much beyond what 4G and 5G already deliver. So at this point future-proofing telecom in the anticipation of tens or more Gbs of consumer bandwidth demand seems like expensive insurance being taken out against an improbable event. As can be seen in the charts belowexcerpted from my book, The End of Telecoms History, and compiled from a mix of sources, including Cisco and Barclays Researcha downward trend in data growth has been evident for at least the past decade. The statistics being tracked in the charts Growth of Mobile-Data Usage and Growth of Landline-Data Usage may seem a little counterintuitive at first. But its important to clarify that these charts do not suggest that overall bandwidth usage is declining. Rather, the conclusion these charts lead to is that the rate of bandwidth growth is slowing. Approaching Zero Growth As mobile-data growth slows, the telecom industry faces a new reality. Current 5G networks black and orange lines and terrestrial broadband networks orange line meet most consumer needs. Providers of both have seen a decrease in the growth of their data usage over recent years. As mobile-data growth slows, the telecom industry faces a new reality. Current 5G networks black and orange lines and terrestrial broadband networks orange line meet most consumer needs. Providers of both have seen a decrease in the growth of their data usage over recent years. Lets start with mobile data. Between 2015 and 2023, theres a consistent decline in bandwidth growth of some 6 percent per year. The overall trend is a little harder to interpret in landline bandwidth data, because theres a large COVID-related peak in 2020 and 2021. But even after accounting for this entirely understandable anomaly, the trend is that home and office broadband growth fell on average by around 3 percent per year between 2015 and 2023. Extrapolating the trends from both of these curves leads to the ultimate conclusion that data growth should ultimately fall to zero or at least a negligibly small number by around 2027. This is an unpopular conclusion. It runs contrary to the persistent drumbeat of a many-Gbs future that telecom experts have been claiming for years. For example, in November 2023 the Biden White House published its spectrum strategy, which states, According to one estimate, data traffic on macro cellular networks is expected to increase by over 250 percent in the next 5 years, and over 500 percent in the next 10 years. Additionally, the Stockholm-based telecom company Ericsson recently predicted near-term surges in mobile data traffic. And the United Kingdoms telecommunications regulator, Ofcom forecast a bandwidth growth-rate of 40 percent for the foreseeable future. But, as shown in the charts here, many mobile and Internet users in the developed world seem to be accessing all the bandwidth they need. Data rates are no longer the constraining and determinative factor that they used to be. The need to continue developing faster and bigger networks may therefore be overplayed today. That chapter of the Internets history is arguably now over, or it soon will be. The implications of having enough coverage and bandwidth are most obvious in the equipment-supply industry. Major network suppliers may need to become accustomed to the new reality of data rates leveling out. Are Ericssons and Nokiasrecent layoffs and the bankruptcies of smaller suppliers such as Airspan Networks a harbinger of whats coming for telecom markets? Operators are already investing less in 5G equipment and are likely already close to maintenance only spending. Most mobile and fixed operators have not seen revenue growth above inflation for many years but hold out hope that somehow this will turn around. Perhaps, though, if the numbers referenced here are to be believed, that turnaround isnt coming. Davide Comai Telecommunications has historically been a high-growth industry, but current trends suggest its heading toward something more staticmore like a public utility, where in this case the public good is delivering data connectivity reliably. Extrapolating these trends, equipment suppliers wont need to invest as much on bandwidth expansion but instead will focus on improving the margins on existing lines of products. Some degree of bandwidth expansion for 6G networks will still be necessary. The metaverse example above suggests a range of ceiling heights in the maximum Gbs that users will demand in the years ahead. For most, 1 Gbs still appears to be more than enough. For those who use high-end applications like future immersive virtual worlds, perhaps that ceiling is closer to 5 Gbs. But concentrating research efforts on 6G deployments that can deliver 10 Gbs and higher for everyone appears not to be grounded in any currently imaginable consumer technologies. To adjust to a potential new reality of operating their wireless networks at closer to utility-like or commodity-like terms, many telecom companies may face a future of restructuring and cost cutting. A useful analogy here are budget airlines, which thrive because most consumers select their airfare on the basis of cost. Similarly, the way for future telecom companies to win a larger share of the customer base may be increasingly dictated not by technological innovation but by price and customer service. To be clear, the need for new telecom research will continue. But with bandwidth expansion deprioritized, other innovations will certainly include cheaper and more efficient or more reliable ways to deliver existing services. If consumer demand for ever more mobile data continues to dry up, regulators would no longer need to find new spectrum bands for cellular every few years and then conduct auctions. Indeed, the demand for spectrum may abate across most areas. Regulators may also have to consider whether fewer operators may be better for a country, with perhaps only a single underlying fixed and mobile network in many placesjust as utilities for electricity, water, gas, and the like are often structured around single or a limited set of operators. Finally, politicians will need to rethink their desire to be at the forefront of metrics such as homes connected by fiber, 5G deployment, or national leadership in 6G. Thats a bit like wanting to be top of the league for the number of Ferraris per capita. Instead, the number of homes with sufficient connectivity and percentage of the country covered by 10 Mbs mobile may be better metrics to pursue as policy goals. Another area of research will surely involve widening coverage in underserved areas and regions of the worldwhile still keeping costs low with more environmentally friendly solutions. Outside of urban areas, broadband is sometimes slow, with mobile connectivity nonexistent. Even urban areas contain so-called not-spots, while indoor coverage can be particularly problematic, especially when the building is clad with materials that are near-impenetrable to radio waves. Broadly, there are two main ways for telecoms to shore up the current digital divide. The first is regulatory. Government funding, whether through new regulation and existing grants already on the books, can go to telecom providers in many regions that have been identified for broadband expansion. Indirect sources of funding should not be overlooked eitherfor instance, to allow operators to retain radio-spectrum license fees and without paying auction fees. The second component is technological. Lower-cost rural telecom deployments could include satellite Internet deployments. Better indoor coverage can happen via private 5G networks or through improved access to existing and enhanced Wi-Fi. The above scenarios represent a major change of directionfrom an industry built around innovating a new mobile generation every decade toward an industry focused on delivering lower prices and increased reliability. The coming 6G age might not be what telecom forecasters imagine. Its dawn may not herald a bold summit push toward 10 Gbs and beyond. Instead, the 6G age could usher in something closer to an adjustment period, with the greatest opportunities for those who best understand how to benefit from the end of the era of rapid bandwidth growth in telecom history. This article appears in the March 2025 print issue as Rethinking 6G. William Webb is an independent consultant providing advice and support across telecommunications matters. He has been a director at Ofcom and was the president of the Institution of Engineering and Technology from 2014 to 2015. He has published 19 books and has multiple honorary doctorates and awards. His latest book, The End of Telecoms History, from which the present article has been adapted, is available on Amazon.com. Good Review! I didnt feel the high rates of 5G than 4G, because the bandwith is far beyond my need. And I think that 6G is likely to move towards large-scale connectivity numbers, which is also defined in 5G-A. There is a new IoT field called Ambient IoT, which is developed for the trillions of IoT nodes. However, is it really necessary for consumers to connect everything, such as books, keys, food, and clothes? This will lead to a new generation of information explosion! We need AI housekeeper to help us filter useless information. 5G has not improved my mobile experience, is it deployed for autonomous driving? The proposition for faster cell service has historically hinged on the need for self-driving cars and utilities to be transmitting data for optimal and safer performance, but the usage data and the nonexistent prospect of broad adoption of self driving cars clearly do not bear that out. I am among many who, like Jeff Hecht, who are overwhelmed by the firehose of media available from phones, computers and streaming devices and turning to books and simpler pleasures. Millenials are doing the same if the sales of record players, CD players and flip phones are any indication.

The March issue of IEEE Spectrum is here! Our favorite jumping robot is hoping for a trip to Saturns icy moon British startup Deep is pioneering a new way to study the ocean Your weekly selection of awesome robot videos Outpace design complexity with shift-left design rule checking methods Your weekly selection of awesome robot videos The Robotics and AI Institute is teaching robot dogs to run and bicycles to jump Your weekly selection of awesome robot videos IITs hydraulic quadruped can carry a pair of massive arms Your weekly selection of awesome robot videos 20 years ago, Stanley won the DARPA Grand Challenge, but the tech is still niche Your weekly selection of awesome robot videos Heres a simple way to identify who, or what, is talking to us A DARPA project overturns longstanding assumptions Your weekly selection of awesome robot videos Unpacking the complexities of energy system transition The robot could more easily study delicate underwater environments Your weekly selection of awesome robot videos The latest developments in aviation, satellites, astrophysics, space flight, and space exploration The latest advances in artificial intelligence and machine learning, generative AI, ChatGPT, LLMs, deepfakes, and more The latest developments in biomedical devices, prosthetics, imaging, and diagnostics The latest developments in renewables, carbon capture, emissions monitoring, geoengineering, and electrifying everything The latest developments in embedded systems, hardware, software, quantum computers, and IT The latest developments in gadgets, gaming, smart phones, virtual reality, and audiovideo The latest developments in power generation, batteries, renewables, smart grid, and nuclear energy A look back at key moments in the history of cyberspace, electronics, and the space age The latest developments in consumer robots, humanoids, drones, and automation The latest developments in processors, memory, nanotechnology, optoelectronics, and integrated circuit design and materials The latest developments in computer vision, lidar, chemical sensors, and remote sensing The latest developments in wireless networks, Internet standards, 5G, IoT, security, and information theory The latest developments in electric vehicles, advanced aircraft, and self-driving cars Reviews, hands-on projects, and the latest on the maker movement from Spectrums DIY editor Stephen Cass Profiles and interviews with engineers, tech industry news, and the latest trends on the job market The latest news about IEEE, its members, tech history, and new offerings

The FDA is poised to approve the notorious party drug as a therapy. Heres what it means, and where similar drugs stand in the US. MIT Technology Reviews Whats Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them here. MDMA, sometimes called Molly or ecstasy, has been banned in the United States for more than three decades. Now this potent mind-altering drug is poised to become a badly needed therapy for PTSD. On June 4, the Food and Drug Administrations advisory committee will meet to discuss the risks and benefits of MDMA therapy. If the committee votes in favor of the drug, it could be approved to treat PTSD this summer. The approval would represent a momentous achievement for proponents of mind-altering drugs, who have been working toward this goal for decades. And it could help pave the way for FDA approval of other illicit drugs like psilocybin. But the details surrounding how these compounds will make the transition from illicit substances to legitimate therapies are still foggy. Heres what to know ahead of the upcoming hearing. Studies suggest the compound can help treat mental-health disorders like PTSD and depression. Lykos, the company that has been developing MDMA as a therapy, looked at efficacy in two clinical trials that included about 200 people with PTSD. Researchers randomly assigned participants to receive psychotherapy with or without MDMA. The group that received MDMA-assisted therapy had a greater reduction in PTSD symptoms. They were also more likely to respond to treatment, to meet the criteria for PTSD remission, and to lose their diagnosis of PTSD. Psychedelic drugs are being pursued as cure-alls for mental-health disorders. But the hype bubble could be about to burst. But some experts question the validity of the results. With substances like MDMA, study participants almost always know whether theyve received the drug or a placebo. That can skew the results, especially when the participants and therapists strongly believe a drug is going to help. The Institute for Clinical and Economic Review ICER, a nonprofit research organization that evaluates the clinical and economic value of drugs, recently rated the evidence for MDMA-assisted therapy as insufficient. In briefing documents published ahead of the June 4 meeting, FDA officials write that the question of approving MDMA presents a number of complex review issues. The ICER report also referenced allegations of misconduct and ethical violations. Lykos formerly the Multidisciplinary Association for Psychedelic Studies Public Benefit Corporation acknowledges that ethical violations occurred in one particularly high-profile case. But in a rebuttal to the ICER report, more than 70 researchers involved in the trials wrote that a number of assertions in the ICER report represent hearsay, and should be weighted accordingly. Lykos did not respond to an interview request. At the meeting on the 4th, the FDA has asked experts to discuss whether Lykos has demonstrated that MDMA is effective, whether the drugs effect lasts, and what role psychotherapy plays. The committee will also discuss safety, including the drugs potential for abuse and the risk posed by the impairment MDMA causes. MDMA is illegal. In 1985, the Drug Enforcement Agency grew concerned about growing street use of the drug and added it to its list of Schedule 1 substancesthose with a high abuse potential and no accepted medical use. MDMA boosts the brains production of feel-good neurotransmitters, causing a burst of euphoria and good will toward others. But the drug can also cause high blood pressure, memory problems, anxiety, irritability, and confusion. And repeated use can cause lasting changes in the brain. That has yet to be determined. It could take months for the DEA to reclassify the drug. After that, its up to individual states. Lykos applied for approval of MDMA-assisted therapy, not just the compound itself. In the clinical trials, MDMA administration happened in the presence of licensed therapists, who then helped patients process their emotions during therapy sessions that lasted for hours. But regulating therapy isnt part of the FDAs purview. The FDA approves drugs it doesnt oversee how theyre administered. The agency has been clear with us, says Kabir Nath, CEO of Compass Pathways, the company working to bring psilocybin to market. They dont want to regulate psychotherapy, because they see that as the practice of medicine, and thats not their job. The words people used to describe their trip experiences could lead to better drugs to treat mental illness. However, for drugs that carry a risk of serious side effects, the FDA can add a risk evaluation and mitigation strategy to its approval. For MDMA that might include mandating that the health-care professionals who administer the medication have certain certifications or specialized training, or requiring that the drug be dispensed only in licensed facilities. For example, Spravato, a nasal spray approved in 2019 for depression that works much like ketamine, is available only at a limited number of health-care facilities and must be taken under the observation of a health-care provider. Having safeguards in place for MDMA makes sense, at least at the outset, says Matt Lamkin, an associate professor at the University of Tulsa College of Law who has been following the field closely. Given the history, I think it would only take a couple of high-profile bad incidents to potentially set things back. Psilocybin, a.k.a. the active ingredient in magic mushrooms. This summer Compass Pathways will release the first results from one of its phase 3 trials of psilocybin to treat depression. Results from the other trial will come in the middle of 2025, whichif all goes wellputs the company on track to file for approval in the fall or winter of next year. With the FDA review and the DEA rescheduling, its still kind of two to three years out, Nath says. Some states are moving ahead without formal approval. Oregon voters made psilocybin legal in 2020, and the drug is now accessible there at about 20 licensed centers for supervised use. Its an adult use program that has a therapeutic element, says Ismail Ali, director of policy and advocacy at the Multidisciplinary Association for Psychedelic Studies MAPS. Colorado voted to legalize psilocybin and some other plant-based psychedelics in 2022, and the state is now working to develop a framework to guide the licensing of facilitators to administer these drugs for therapeutic purposes. More states could follow. Maybe. The DEA can still prosecute physicians if theyre prescribing drugs outside of their medically accepted uses. But Lamkin does see the lines between recreational use and medical use getting blurry. What were seeing is that the therapeutic uses have recreational side effects and the recreation has therapeutic side effects, he says. Im interested to see how long they can keep the genie in the bottle. Last summer, Australia became the first country to approve MDMA and psilocybin as medicines to treat psychiatric disorders, but the therapies are not yet widely available. The first clinic opened just a few months ago. The US is poised to become the second country if the FDA greenlights Lykoss application. Health Canada told the CBC it is watching the FDAs review of MDMA with interest. Europe is lagging a bit behind, but there are some signs of movement. In April, the European Medicines Agency convened a workshop to bring together a variety of stakeholders to discuss a regulatory framework for psychedelics. The company is making a foray into scientific discovery with an AI built to help manufacture stem cells. Frozen embryos are filling storage banks around the world. Its a struggle to know what to do with them. Its a new way to create bi-paternal mice that can survive to adulthoodbut human applications are still a long way off. More volunteers will get Elon Musks brain implant, but dont expect a product soon. Discover special offers, top stories, upcoming events, and more. Thank you for submitting your email! It looks like something went wrong. Were having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-servicetechnologyreview.com with a list of newsletters youd like to receive.  2025 MIT Technology Review

The awards honor work on gene regulation and the relationship between political systems and economic growth. Two MIT professors, an alumnus, and a former postdoc are among the winners of 2024s Nobel Prizes. Professors Daron Acemoglu and Simon Johnson, PhD 89, shared the prize in economics with political scientist James Robinson of the University of Chicago, with whom they have long collaborated. Using evidence from the last 500 years, their work has empirically demonstrated that inclusive governments such as democracies, which extend individual rights and political liberties while upholding the rule of law, have generated greater economic activity than extractive political systems, where power is wielded by a small elite. Partly because economic growth depends on technological innovation, it is best sustained when countries protect property rights, giving more people the incentive to invent things. Acemoglu, an Institute Professor, has been a member of the MIT faculty since 1993. Johnson, the Ronald A. Kurtz Professor of Entrepreneurship at MIT Sloan, was chief economist of the International Monetary Fund from 2007 to 2008. Meanwhile, Victor Ambros 75, PhD 79, a professor at the University of Massachusetts Chan Medical School, and Gary Ruvkun, a professor at Harvard Medical School and Massachusetts General Hospital, shared the prize in medicine for their discovery of microRNA, a class of tiny RNA molecules that help govern gene regulation. This crucial mechanism allows cells with the same chromosomes to develop into cell types with different characteristics and functions. The foundation for their discoveries was laid by their work on mutant forms of the roundworm C. elegans as MIT postdocs in the lab of Professor H. Robert Horvitz who would win a Nobel in 2002. Later, working independently, they showed that a certain roundworm gene produces a very short RNA molecule that binds to messenger RNA encoding a different gene and blocks it from being translated into protein. Since then, more than 1,000 microRNA genes have been found in humans. In an interview with the Journal of Cell Biology, Ambros also credited the contributions of collaborators including his wife, Rosalind Candy Lee 76, and postdoc Rhonda Feinbaum. This story was part of our JanuaryFebruary 2025 issue. Despite fewer clicks, copyright fights, and sometimes iffy answers, AI could unlock new ways to summon all the worlds knowledge. With a new reasoning model that matches the performance of ChatGPT o1, DeepSeek managed to turn restrictions into innovation. The company is making a foray into scientific discovery with an AI built to help manufacture stem cells. A string of startups are racing to build models that can produce better and better software. They claim its the shortest path to AGI. Discover special offers, top stories, upcoming events, and more. Thank you for submitting your email! It looks like something went wrong. Were having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-servicetechnologyreview.com with a list of newsletters youd like to receive.  2025 MIT Technology Review

Shan built Glaze and Nightshade, two tools that help artists protect their copyright. Shawn Shan is one of MIT Technology Reviews 2024 Innovators Under 35. Meet the rest of this years honorees. When image-generating models such as DALL-E 2, Midjourney, and Stable Diffusion kick-started the generative AI boom in early 2022, artists started noticing odd similarities between AI-generated images and those theyd created themselves. Many found that their work had been scraped into massive data sets and used to train AI models, which then produced knockoffs in their creative style. Many also lost work when potential clients used AI tools to generate images instead of hiring artists, and others were asked to use AI themselves and received lower rates. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. Now artists are fighting back. And some of the most powerful tools they have were built by Shawn Shan, 26, a PhD student in computer science at the University of Chicago and MIT Technology Reviews 2024 Innovator of the Year. Shan got his start in AI security and privacy as an undergraduate there and participated in a project that built Fawkes, a tool to protect faces from facial recognition technology. But it was conversations with artists who had been hurt by the generative AI boom that propelled him into the middle of one of the biggest fights in the field. Soon after learning about the impact on artists, Shan and his advisors Ben Zhao who made our Innovators Under 35 list in 2006 and Heather Zheng who was on the 2005 list decided to build a tool to help. They gathered input from more than a thousand artists to learn what they needed and how they would use any protective technology. Shan coded the algorithm behind Glaze, a tool that lets artists mask their personal style from AI mimicry. Glaze came out in early 2023, and last October, Shan and his team introduced another tool called Nightshade, which adds an invisible layer of poison to images to hinder image-generating AI models if they attempt to incorporate those images into their data sets. If enough poison is drawn into a machine-learning models training data, it could permanently break models and make their outputs unpredictable. Both algorithms work by adding invisible changes to the pixels of images that disrupt the way machine-learning models interpret them. The response to Glaze was both overwhelming and stressful, Shan says. The team received backlash from generative AI boosters on social media, and there were several attempts to break the protections. But artists loved it. Glaze has been downloaded nearly 3.5 million times and Nightshade over 700,000. It has also been integrated into the popular new art platform Cara, allowing artists to embed its protection in their work when they upload their images. And Glaze received a distinguished paper award and the Internet Defense Prize at the Usenix Security Symposium, a top computer security conference Shans work has also allowed artists to be creative online again, says Karla Ortiz, an artist who has worked with him and the team to build Glaze and is part of a class action lawsuit against generative AI companies for copyright violation. Meet the rest of this years Innovators Under 35. They do it because theyre passionate about a community thats been  taken advantage of and exploited, and theyre just really invested in it, says Ortiz. It was Shan, Zhao says, who first understood what kinds of protections artists were looking for and realized that the work they did together on Fawkes could help them build Glaze. Zhao describes Shans technical abilities as some of the strongest hes ever seen, but what really sets him apart, he says, is his ability to connect dots across disciplines. These are the kinds of things that you really cant train, Zhao adds. Shan says he wants to tilt the power balance back from large corporations to people. Right now, the AI powerhouses are all private companies, and their job is not to protect people and society, he says. Their job is to make shareholders happy. He aims to show, through his work on Glaze and Nightshade, that AI companies can collaborate with artists and help them benefit from AI or empower them to opt out. Some firms are looking into how they could use the tools to protect their intellectual property. Next, Shan wants to build tools to help regulators audit AI models and enforce laws. He also plans to further develop Glaze and Nightshade in ways that could make them easier to apply to other industries, such as gaming, music, or journalism. I will be in this project for life, he says. Watch Shan talk about whats next for his work in a recent interview by Amy Nordrum, MIT Technology Reviews executive editor. This story has been updated. This story was part of our SeptemberOctober 2024 issue. Despite fewer clicks, copyright fights, and sometimes iffy answers, AI could unlock new ways to summon all the worlds knowledge. With a new reasoning model that matches the performance of ChatGPT o1, DeepSeek managed to turn restrictions into innovation. A string of startups are racing to build models that can produce better and better software. They claim its the shortest path to AGI. You already know that agents and small language models are the next big things. Here are five other hot trends you should watch out for this year. Discover special offers, top stories, upcoming events, and more. Thank you for submitting your email! It looks like something went wrong. Were having trouble saving your preferences. Try refreshing this page and updating them one more time. If you continue to get this message, reach out to us at customer-servicetechnologyreview.com with a list of newsletters youd like to receive.  2025 MIT Technology Review

Opinion Just as Half-Life 2 helped launch Steam, a sequel could help establish non-Windows PC gaming. A little over 20 years ago, Valve was getting ready to release a new Half-Life game. At the same time, the company was trying to push Steam as a new option for players to download and update games over the Internet. Requiring Steam in order to play Half-Life 2 led to plenty of grumbling from players in 2004. But the high-profile Steam exclusive helped build an instant user base for Valves fresh distribution system, setting it on a path to eventually become the unquestioned leader in the space. The link between the new game and the new platform helped promote a bold alternative to the retail game sales and distribution systems that had dominated PC gaming for decades. Today, all indications suggest that Valve is getting ready to release a new Half-Life game. At the same time, the company is getting ready to push SteamOS as a new option for third-party hardware makers and individual users to download and test themselves. Requiring SteamOS to play Half-Life 3 would definitely lead to a lot of grumbling from players. But the high-profile exclusive could help build an instant user base for Valves fresh operating system, perhaps setting it on the path to become the unquestioned leader in the space. A link between the new game and the new platform could help promote a bold alternative to the Windows-based systems that have dominated PC gaming for decades. Getting players to change the established platform they use to buy and play games either in terms of hardware or software usually requires some sort of instantly apparent benefit for the player. Those benefits can range from the tangible e.g., an improved controller, better graphics performance to the ancillary e.g., social features, achievements to the downright weird e.g., a second screen on a portable. Often, though, a core reason why players switch platforms is for access to exclusive system seller games that arent available any other way. Half-Life 2s role in popularizing early Steam shows just how much a highly anticipated exclusive can convince otherwise reluctant players to invest time and effort in a new platform. To see what can happen without such an exclusive, we only need to look to Valves 2015 launch of the Steam Machine hardware line, powered by the first version of the Linux-based SteamOS. At the time, Valve was selling SteamOS mainly as an alternative to a new Windows 8 environment that Valve co-founder Gabe Newell saw as a catastrophe in the making for the PC gaming world. Newell described SteamOS as a hedging strategy against Microsofts potential ability to force all Windows 8 app distribution through the Windows Store, a la Apples total control of iPhone app distribution. When Microsoft failed to impose that kind of hegemonic control over Windows apps and games, Valve was left with little else to convince players that it was worth buying a Windows-free Steam Machine or going through the onerous process of installing the original SteamOS on their gaming rigs. Sure, using SteamOS meant saving a few bucks on a Windows license. But it also meant being stuck with an extremely limited library of Linux ports especially when it came to releases from major publishers and poor technical performance compared to Windows even when those ports were available. Given those obvious downsidesand the lack of any obvious upsidesits no wonder that users overwhelmingly ignored SteamOS and Steam Machines at the time. But as we argued way back in 2013, a major exclusive on the scale of Half-Life 3 could have convinced a lot of gamers to overlook at least some of those downsides and give the new platform a chance. Fast forward to today, and the modern version of SteamOS is in a much better place than the Steam Machine-era version ever was. Thats thanks in large part to Valves consistent work on the Proton compatibility layer, which lets the Linux-based SteamOS run almost any game thats designed for Windows with only a few major exceptions. That wide compatibility has been a huge boon for the Steam Deck, which offered many players easy handheld access to vast swathes of PC gaming for the first time. The Steam Deck also showed off SteamOSs major user interface and user experience benefits over clunkier Windows-based gaming portables. Still, the benefits of switching from Windows to SteamOS might seem a bit amorphous to many players today. If Valve is really interested in pushing its OS as an alternative to Windows gaming, a big exclusive game is just the thing to convince a critical mass of players to make the leap. And when it comes to massive PC gaming exclusives, it doesnt get much bigger than the long, long-awaited Half-Life 3. We know it might sound ludicrous to suggest that Valves biggest game in years should ignore the Windows platform thats been used by practically every PC gamer for decades. Keep in mind, though, that there would be nothing stopping existing Windows gamers from downloading and installing a free copy of the Linux-based SteamOS likely on a separate drive or partition to get access to Half-Life 3. Yes, installing a new operating system especially one based on Linux is not exactly a plug-and-play process. But Valve has a long history of streamlining game downloads, updates, and driver installations through Steam itself. If anyone can make the process of setting up a new OS relatively seamless, its Valve. And lets not forget that millions of gamers already have easy access to SteamOS through Steam Deck hardware. Those aging Steam Decks might not be powerful enough to run a game like Half-Life 3 at maximum graphics settings, but Valve games have a history of scaling down well on low-end systems. Valves leaked Powered by SteamOS initiative also seems poised to let third-party hardware makers jump in with more powerful and more Half-Life 3-capable desktops, laptops, and handhelds with SteamOS pre-installed. And thats before we even consider the potential impact of a more powerful Steam Deck 2, which Valves Pierre-Loup Griffais said in 2023 could potentially come in the next couple of years. Tying a major game like Half-Life 3 to a completely new and largely untested operating system would surely lead to some deafening pushback from gamers happy with the Windows-based status quo. An exclusive release could also be risky if SteamOS ends up showing some technical problems as it tries to grow past its Steam Deck roots Linux doesnt exactly have the best track record when it comes to things like game driver compatibility across different hardware. Despite all that, were pretty confident that the vast majority of players interested in Half-Life 3 would jump through a few OS-related hoops to get access to the game. And many of those players would likely stick with Valves gaming-optimized OS going forward rather than spending money on another Windows license. Even a timed exclusivity window for Half-Life 3 on SteamOS could push a lot of early adopters to see what all the fuss is about without excluding those who refuse to switch away from Windows. Failing even that, maybe a non-exclusive Half-Life 3 could be included as a pre-installed freebie with future versions of SteamOS, as an incentive for the curious to try out a new operating system. With the coming wide release of SteamOS, Valve has a rare opportunity to upend the PC gaming OS dominance that Microsoft more or less stumbled into decades ago. A game like Half-Life 3 could be just the carrot needed to get PC gaming as a whole over its longstanding Windows dependence. Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you dont need to know everything, only whats important.

The app will reportedly offer contextual suggestions as you use the phone. Googles AI ambitions know no bounds. A new report claims Googles next phones will herald the arrival of a feature called Pixel Sense that will ingest data from virtually every Google app on your phone, fueling a new personalized experience. This app could be the premiere feature of the Pixel 10 series expected out late this year. According to a report from Android Authority, Pixel Sense is the new name for Pixie, an AI that was supposed to integrate with Google Assistant before Gemini became the center of Googles universe. In late 2023, it looked as though Pixie would be launched on the Pixel 9 series, but that never happened. Now, its reportedly coming back as Pixel Sense, and we have more details on how it might work. Pixel Sense will apparently be able to leverage data you create in apps like Calendar, Gmail, Docs, Maps, Keep Notes, Recorder, Wallet, and almost every other Google app. It can also process media files like screenshots in the same way the Pixel Screenshots app currently does. The goal of collecting all this data is to help you complete tasks faster by suggesting content, products, and names by understanding the context of how you use the phone. Pixel Sense will essentially try to predict what you need without being prompted. Samsung is pursuing a goal that is ostensibly similar to Now Brief, a new AI feature available on the Galaxy S25 series. Now Brief collects data from a handful of apps like Samsung Health, Samsung Calendar, and YouTube to distill your important data with AI. However, it rarely offers anything of use with its morning, noon, and night Now Bar updates. Pixel Sense sounds like a more expansive version of this same approach to processing user dataand perhaps the fulfillment of Google Nows decade-old promise. The supposed list of supported apps is much larger, and theyre apps people actually use. If pouring more and more data into a large language model leads to better insights into your activities, Pixel Sense should be better at guessing what youll need. Admittedly, thats a big if. In a separate leak, images purported to be from a test version of Pixel Sense were posted to a Telegram chat. They show several pages of text that emphasize the apps ability to help you complete tasks faster. Previously, Pixel Sense ne Pixie was shaping up to be an enhanced version of Assistant exclusively for Pixel phonesthis was before Google aggressively pushed Assistant to the side in order to make Gemini the default assistant across its portfolio. The Gemini brand didnt even exist the last time Pixie was in the news, but now it sounds like another application for Googles rapidly evolving on-device AI capabilities. The idea of Googles AI snooping through all your app data probably makes your skin crawlfair. There are reasons to be skeptical of this level of access, but the report claims Pixel Sense will rely on local AI processing just like Samsungs Now Brief. So, instead of piping all your app data into a black box cloud server, the content remains on your phone to be processed by the Gemini Nano model in a more secure manner. It would also work offline. The reappearance of Pixel Sense suggests Google is planning to vastly expand the AI processing that happens in your pocket. Googles mobile AI journey began modestly, with features like Recorder summaries and Magic Compose AI text rewriting plugged into the Gemini Nano model. With each phone release and Pixel Drop, Google adds a few more features and apps powered by on-device AI. In fact, the latest Pixel Drop just added scam detection to the list of features that plug into Gemini and the Tensor chip in Googles latest phones. However, its impossible to know how good or useful this feature will be. Samsungs Now Brief, for example, was largely disappointing, and that phone runs a version of Gemini Nano for on-device processing. Google regularly makes vague claims about how capable Gemini Nano is, but we only have Googles word to go on here. Third-party support for Gemini and NPU hardware is still lacking, with most parts of Googles AI Edge SDK still at the preview level only. That leaves OEMs like Google and Samsung to show us what on-device AI can do. So far, it hasnt been very impressive. Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you dont need to know everything, only whats important.

A zero gravity training sphere makes an appearance, obviously. Most readers come away from George Orwells classic dystopian novel 1984 with the same singular desire to inhabit the world of the book by playing a late 90s first-person puzzle-adventure PC game that includes a zero-g training sphere for some reason. In 1998, publisher MediaX set out to satisfy that widespread literary desire with Big Brother, an officially licensed sequel game set in the 1984 universe. After appearing as a demo at E3 1998 and receiving some scattered press coverage, the Big Brother project fell apart before the game could see a full release. Now, though, you can experience a small taste of this ill-fated literary sequel thanks to a newly unearthed demo that was recovered and posted to the Internet Archive over the weekend. The Lost Media Wiki has a bit more info on the history of Big Brother, which was announced in May 1998 as the first game ever from multimedia CD-ROM maker MediaX. In that announcement, the company said the game would move focus away from 1984s Winston Smith and to new character Eric Blair, whos on a search for his missing fiance Emma sure, why not in a completely changed world dominated by the Thought Police. The pre-rendered introduction for the demo ignores the missing fiance plotline entirely and instead places Eric in the center of a resistance movement on the run from the Thought Police Eric, the Thought Police have been tracking down our brotherhood leaders in hopes of destroying our resistance movement. We have only a few hours left, we must do something to get the police off our trail. As a MiniPac soldier, you should be able to get into the ministry and create a diversion. The bigger the distraction you can create, the better. Its unclear how any diversion would be enough to sufficiently distract the all-encompassing monitoring and thought control network that Orwell describes in 1984, but we digress. The Big Brother announcement promised the ability to interact with everything and disable and destroy intrusive tele-screens and spy cameras watching the players every move across 10 square blocks of Orwells retro-futuristic world. But footage from the demo falls well short of that promise, instead covering some extremely basic Riven-style puzzle gameplay flips switches to turn on the power use a screwdriver to open the grate, etc. played from a first-person view. It all builds up to a sequence where according to a walk-through included on the demo disc you have to put on a zero-g suit before planting a bomb inside a zero gravity training sphere guarded by robots. Sounds like inhabiting the world of the novel to us! Aside from the brief mentions of the Thought Police and MiniPac, the short demo does include a few other incidental nods to its licensed source material, including a WAR IS PEACE propaganda banner and an animated screen with the titular Big Brother seemingly looking down on you. Still, the entire gameplay scenario is so far removed from anything in the actual 1984 novel to make you wonder why they bothered with the license in the first place. Of course, MediaX answers that question in the games announcement, predicting that while the game stands on its own as an entirely new creation in itself and will attract the typical game audience, the Big Brother game will undoubtedly also attract a large literary audience. We sadly never got the chance to see how that large literary audience would have reacted to a game that seemed poised to pervert both the name and themes of 1984 so radically. In any case, this demo can now sit alongside the release of 1984s Fahrenheit 451 and 1992s The Godfather The Action Game on any list of the most questionable game adaptations of respected works of art. Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you dont need to know everything, only whats important.

Googles Project Astra demo is almost ready for prime time. At Mobile World Congress, Google confirmed that a long-awaited Gemini AI feature it first teased nearly a year ago is ready for launch. The companys conversational Gemini Live will soon be able to view live video and screen sharing, a feature Google previously demoed as Project Astra. When Geminis video capabilities arrive, youll be able to simply show the robot something instead of telling it. Right now, Googles multimodal AI can process text, images, and various kinds of documents. However, its ability to accept video as an input is spotty at bestsometimes it can summarize a YouTube video, and sometimes it cant, for unknown reasons. Later in March, the Gemini app on Android will get a major update to its video functionality. Youll be able to open your camera to provide Gemini Live a video stream or share your screen as a live video, thus allowing you to pepper Gemini with questions about what it sees. It can be hard to keep track of which Google AI project is whichthe 2024 Google IO was largely a celebration of all things Gemini AI. The Astra demo made waves as it demonstrated a more natural way to interact with the AI. In the original video, which you can see below, Google showed how Gemini Live could answer questions in real time as the user swept a phone around a room. It had things to say about code on a computer screen, how speakers work, and a network diagram on a whiteboard. It even remembered where the user left their glasses from an earlier part of the video. We dont yet know if the version of this technology thats coming to the Gemini app will be as capable as Googles clearly staged IO demo. However, Google is talking a big game in advance of Gemini Lives upgrade. The company claims this update to the Gemini 2.0 platform will finally make Gemini a true assistant. Google suggests you could use Gemini Lives video chops to have an informative conversation with the robot while you explore new places or get help with piecing together an outfit by sharing your screen while online shopping. The more powerful Gemini Live will arrive on Android phones in the Gemini app later this month. Piping a continuous stream of video into the model is undoubtedly going to consume much more processing than churning through some text. The video features will be part of Gemini Advanced, so youll need to be on the 20 per month AI Premium plan. That subscription also provides access to Googles largest and most expensive AI models. Even with the subscription requirements, Gemini Live will probably lose Google even more money with this update. No company has cracked the code on making money from generative AI just yet. Adding video to the mix will only deepen Googles losses on each user, but this is the kind of feature that could get people to use Gemini more. Securing market share versus the likes of OpenAI is seemingly worth the short-term loss. Despite Googles enormous mobile footprint, its monthly Gemini usage numbers are only in the tens of millions, which is an order of magnitude lower than OpenAIs tools. Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you dont need to know everything, only whats important.

The base iPad also doubled its storage, is 6x faster than some Android models. Its not the update that most Apple watchers were hoping for, but today, Apple announced upgrades to two of its iPads, including the standard iPad and the quirky middle child, the iPad Air. The iPad Air moves up from an M2 in last years Air refresh to an M3 chip, which is capable of Apple Intelligence and more graphics rendering. The Magic Keyboard gets a new layout with a larger trackpad and a physical function key row. Thats a notable upgrade for anybody doing real, portable-minded work with an iPad, but theres even better news The new Magic Keyboard with those physical keys is backward-compatible with previous iPad Airs 4th and 5th generation, 11-inch M2 and M3, and 13-inch M2 and M3 models better news, that is, if youre good with the 270 price. The new Airs ship in the same 11- and 13-inch sizes. Apples messaging about the new Air promotes the graphic performance of the M3 chip, saying it provides up to 40 percent faster graphics performance over M1 and support for things like hardware-accelerated mesh shading and ray tracing. Perhaps more realistically than promising serious gaming on native iPad apps, Apple touts 4 times the performance of an M1 iPad Air for graphics-intensive rendering workflows. Credit Apple The standard iPad, now with an A16 chip and 128GB base storage.The base iPad also got a boost today, earning an entire paragraph in Apples larger Air news release. It jumps up from an A14 to A16 chip inside and doubles its base storage from 64GB to 128GB. Apple cannot help but suggest that its 6x faster than the best-selling Android tablet, which the company does not name a footnote says the tablet is based on a Qualcomm SM6375 chip, so its almost certainly some kind of Samsung Galaxy device. The standard iPad, new iPad Airs, and updated Magic Keyboard will be available on March 12. The iPad is 349 with 128GB storage, and the iPad Airs start at 599 and 799 for the 11- and 13-inch models, respectively. This post was updated to clarify a point about Apple Intelligence availability on iPad Air models. Ars Technica has been separating the signal from the noise for over 25 years. With our unique combination of technical savvy and wide-ranging interest in the technological arts and sciences, Ars is the trusted source in a sea of information. After all, you dont need to know everything, only whats important.

